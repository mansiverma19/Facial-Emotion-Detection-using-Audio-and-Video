{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b15075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e8d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Neutral','Disgust','Fear','Sadness', 'Anger', 'Happiness', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d829f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path=os.path.join('FER','workspace')\n",
    "video_path=os.path.join(root_path,'video')\n",
    "audio_path=os.path.join('FER','audio')\n",
    "model_path=train=os.path.join(root_path,'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15640220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models (replace these with your actual models)\n",
    "audio_model = load_model(os.path.join(model_path,'audio-4-89','cnn_model_audio-4(acc=89.5,val=52).h5'))\n",
    "image_model = load_model(os.path.join(model_path,'3-resnet50-99.3%','3-resnet50.h5'))\n",
    "faceCascade = cv2.CascadeClassifier (cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df047a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(frame,emotion,prediction,bb_lst):\n",
    "    x=bb_lst[0]\n",
    "    y=bb_lst[1]\n",
    "    w=bb_lst[2]\n",
    "    h=bb_lst[3]\n",
    "    font_scale=1.5\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "    prediction = str(prediction)\n",
    "    if (emotion==\"Neutral\"): \n",
    "        status = \"Neutral\"+prediction\n",
    "        x1,y1, w1,h1 = 0,0,175,75\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1+ w1, y1+ h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "#         cv2.putText(frame, status, (x1+ int(w1/10),y1 +int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "    elif (emotion==\"Disgust\"):\n",
    "        status = \"Disgust\"+prediction\n",
    "        x1,y1,w1,h1 =0,0,175,75\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "#         cv2.putText(frame, status, (x1+ int(w1/10),y + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "\n",
    "    \n",
    "    elif (emotion==\"Fear\"): \n",
    "        status = \"Fear\"+prediction\n",
    "        x1,y1,w1,h1=0,0,175,75\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "#         cv2.putText(frame, status, (x1+ int(w1/10),y1+ int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "\n",
    "    elif (emotion==\"Sadness\"):\n",
    "        status = \"Sadness\"+prediction\n",
    "        x1,y1,w1,h1 = 0,0,175,75\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "#         cv2.putText(frame, status, (x1+ int(w1/10),y1+ int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "\n",
    "        \n",
    "    elif (emotion==\"Anger\"):\n",
    "        status=\"Anger\"+prediction\n",
    "        x1,y1, w1,h1 = 0,0,175,75\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "#         cv2.putText(frame, status, (x1+ int(w1/10), y1 + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "\n",
    "\n",
    "    elif (emotion==\"Happiness\"):\n",
    "        status = \"Happiness\"+prediction\n",
    "        x1,y1,w1,h1 = 0,0,175,75\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "#         cv2.putText(frame, status, (x1+ int(w1/10),y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "        cv2.rectangle (frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "\n",
    "    else:\n",
    "        status = \"Surprise\"+prediction\n",
    "        x1,y1,w1,h1 = 0,0,175,75\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "#         cv2.putText(frame, status, (x1+ int(w1/10),y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "        cv2.rectangle (frame, (x, y), (x+w, y+h), (0, 0, 255))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91842a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(X,sample_rate, mfcc, chroma, mel): \n",
    "    if chroma:\n",
    "        stft=np.abs(librosa.stft(X))\n",
    "    result=np.array([])\n",
    "    if mfcc:\n",
    "        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0) \n",
    "        result=np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0731cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process video data\n",
    "def process_video(frame):\n",
    "    gray= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces= faceCascade.detectMultiScale (gray, scaleFactor=1.1, minNeighbors=4)\n",
    "    dct = { 0  :  'Neutral',\n",
    "            1  : 'Disgust',\n",
    "            2  : 'Fear',\n",
    "            3  : 'Sadness',\n",
    "            4  : 'Anger',\n",
    "            5  : 'Happiness',\n",
    "            6  : 'Surprise'}\n",
    "    \n",
    "    # Adjust this value based on your needs\n",
    "    min_confidence = 30  \n",
    "\n",
    "    # Filter faces based on confidence\n",
    "    filtered_faces = [face for face in faces if face[2] >= min_confidence]\n",
    "\n",
    " \n",
    "    for x,y,w,h in faces:\n",
    "        roi_gray =gray[y:y+h, x:x+w]\n",
    "        roi_color = frame [y: y+h, x:x+w]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale (roi_gray)\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi= roi_color[ey: ey+eh, ex:ex + ew] ## cropping the face\n",
    "                \n",
    "    final_image = cv2.resize(face_roi, (224,224))\n",
    "    final_image = np.expand_dims(final_image, axis=0)\n",
    "    final_image = final_image/255.0\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    Predictions = image_model.predict(final_image)\n",
    "    \n",
    "    bb_lst=[x,y,w,h]\n",
    "    idx = np.argmax(Predictions)\n",
    "    emotion = dct[idx]\n",
    "    max_pred = np.max(Predictions)\n",
    "    #print('video max :',max_pred)\n",
    "    return emotion,max_pred,bb_lst\n",
    "\n",
    "# Function to process audio data\n",
    "def process_audio(audio_data,sr):\n",
    "#     sr, audio_data = wavfile.read(data)\n",
    "    result = extract_feature(audio_data,sr, mfcc=True, chroma=True, mel=True)\n",
    "    result=result.tolist()\n",
    "    x=[]\n",
    "    x.append(result)\n",
    "    x=np.array(x)\n",
    "    \n",
    "    dct = { 0  :  'Anger',\n",
    "            1  : 'Disgust',\n",
    "            2  : 'Fear',\n",
    "            3  : 'Happiness',\n",
    "            4  : 'Neutral',\n",
    "            5  : 'Sadness',\n",
    "            6  : 'Surprise'}\n",
    "    \n",
    "    # Make predictions using the audio model\n",
    "    audio_prediction = audio_model.predict(x)\n",
    "#     print(audio_prediction[0],type(audio_prediction[0]))\n",
    "    idx=np.argmax(audio_prediction)\n",
    "    max_pred=np.max(audio_prediction)\n",
    "    emotion=dct[idx]\n",
    "    return emotion,max_pred\n",
    "\n",
    "# Function to integrate video and audio predictions\n",
    "def pipeline(video_data, audio_data,sr):\n",
    "    video_emotion,video_prediction,bb_lst= process_video(video_data)\n",
    "    audio_emotion,audio_prediction = process_audio(audio_data,sr)\n",
    "    \n",
    "    print('video_prediction',video_prediction)\n",
    "    print('audio_prediction',audio_prediction)\n",
    "\n",
    "    if video_emotion==audio_emotion:\n",
    "        if video_prediction > audio_prediction:\n",
    "            return video_emotion,video_prediction,bb_lst\n",
    "        else:\n",
    "            return audio_emotion,audio_prediction,bb_lst\n",
    "    else:\n",
    "        return video_emotion,video_prediction,bb_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e34c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not detected\n",
      "Face not detected\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'face_roi' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m audio_data_float \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mbuf_to_float(audio_data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#emotion=process_audio(audio_data_float,audio_fs)\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m emotion,prediction,bb_lst \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43maudio_data_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43maudio_fs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(emotion,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m :\u001b[39m\u001b[38;5;124m\"\u001b[39m,prediction)\n\u001b[0;32m     42\u001b[0m draw_rectangle(frame,emotion,prediction,bb_lst)\n",
      "Cell \u001b[1;32mIn[7], line 72\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(video_data, audio_data, sr)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipeline\u001b[39m(video_data, audio_data,sr):\n\u001b[1;32m---> 72\u001b[0m     video_emotion,video_prediction,bb_lst\u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     audio_emotion,audio_prediction \u001b[38;5;241m=\u001b[39m process_audio(audio_data,sr)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m,video_prediction)\n",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (ex, ey, ew, eh) \u001b[38;5;129;01min\u001b[39;00m facess:\n\u001b[0;32m     29\u001b[0m             face_roi\u001b[38;5;241m=\u001b[39m roi_color[ey: ey\u001b[38;5;241m+\u001b[39meh, ex:ex \u001b[38;5;241m+\u001b[39m ew] \u001b[38;5;66;03m## cropping the face\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m final_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(\u001b[43mface_roi\u001b[49m, (\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m     32\u001b[0m final_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(final_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     33\u001b[0m final_image \u001b[38;5;241m=\u001b[39m final_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'face_roi' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up audio stream parameters\n",
    "audio_fs = 44100\n",
    "audio_channels = 1\n",
    "audio_dtype = pyaudio.paInt16  # Use pyaudio int16 type\n",
    "audio_chunk_size = 1024\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Open audio stream\n",
    "stream = p.open(format=audio_dtype,\n",
    "                channels=audio_channels,\n",
    "                rate=audio_fs,\n",
    "                input=True,\n",
    "                frames_per_buffer=audio_chunk_size)\n",
    "\n",
    "# open video stream\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a real-time audio feature extraction loop\n",
    "try:\n",
    "    while True:\n",
    "       \n",
    "        ret, frame = video_capture.read()\n",
    "        \n",
    "        # Read audio data from the stream\n",
    "        audio_data = np.frombuffer(stream.read(audio_chunk_size), dtype=np.int16)\n",
    "        \n",
    "        # Convert audio data to floating-point format\n",
    "        audio_data_float = librosa.util.buf_to_float(audio_data, dtype=np.float32)\n",
    "        #emotion=process_audio(audio_data_float,audio_fs)\n",
    "        \n",
    "        emotion,prediction,bb_lst = pipeline(frame,audio_data_float,audio_fs)\n",
    "       \n",
    "        print(emotion,\" :\",prediction)\n",
    "        draw_rectangle(frame,emotion,prediction,bb_lst)\n",
    "        cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "        # Break the loop when 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Stop the audio stream and close PyAudio\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15682a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod1",
   "language": "python",
   "name": "tfod1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
